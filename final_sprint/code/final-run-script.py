# -*- coding: utf-8 -*-
"""project_assignment_5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XjOLFqW_iwgiJQ428cgR9PRdmebyiAc3
"""

#Installing Java and Spark

# !apt-get update -qq
# !pip install -q spark
# !apt-get install -y openjdk-11-jdk-headless -qq

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = (SparkSession.builder
         .appName("CS131-24")
         .config("spark.sql.shuffle.partitions", "8")
        #  .master("local[*]")
         .getOrCreate())

bucket = "cs-133-fp-1" # replace with your bucket name
data_path = f"gs://{bucket}/data/synthetic_personal_finance_dataset.csv"

finance_df = (spark.read.option("header", True).option("inferSchema", True).csv(data_path))

print("DONE.  Rows:", finance_df.count())
finance_df.printSchema()
finance_df.show(5)

out_dir = f"gs://{bucket}/output/"
# os.makedirs(out_dir, exist_ok=True)

#Data Cleaning
finance_df = spark.read.csv(data_path, header=True, inferSchema=True)

#Data Cleaning
df = finance_df.toDF(*[c.strip().lower().replace(" ", "_") for c in finance_df.columns])

#Data Cleaning
for f in df.schema.fields:
    if str(f.dataType) == 'StringType':
        df = df.withColumn(f.name, F.trim(F.col(f.name)))

# Parse record_date
df = df.withColumn("record_date_parsed",
                   F.coalesce(
                       F.to_date(F.col("record_date"), "yyyy-MM-dd"),
                       F.to_date(F.col("record_date"), "MM/dd/yyyy"),
                       F.to_date(F.col("record_date"), "M/d/yyyy"),
                       F.to_date(F.col("record_date"), "dd-MM-yyyy"),
                       F.to_date(F.col("record_date"), "d-M-yyyy")
                   ))

# Convert numeric columns
numeric_cols = ["monthly_income_usd", "monthly_expenses_usd", "savings_usd",
                "loan_amount_usd", "monthly_emi_usd", "loan_interest_rate_pct",
                "debt_to_income_ratio", "credit_score", "savings_to_income_ratio"]
for col in numeric_cols:
    df = df.withColumn(col, F.col(col).cast("double"))

# Fill missing any missing categorical columns
cat_cols = ["gender", "education_level", "employment_status", "job_title", "loan_type", "region"]
for col in cat_cols:
    df = df.fillna({col: "unknown"})

# Extract year_month for monthly aggregation
df = df.withColumn("year_month", F.date_format("record_date_parsed", "yyyy-MM"))

from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType

exchange_rates = {
    "North America": 1.0,
    "Europe": 1.16,
    "Africa": 0.058,
    "Asia": 0.141,
    "Other": 0.438,
}

def convert_to_usd(region, value):
    if value is None:
        return None
    rate = exchange_rates.get(region, 1.0)
    return float(value) * rate

convert_udf = udf(convert_to_usd, DoubleType())

df = df.withColumn("monthly_income_usd", convert_udf("region", "monthly_income_usd"))
df = df.withColumn("savings_usd", convert_udf("region", "savings_usd"))

# GROUP BY & AGG
education_agg = (df.groupBy("education_level")
                 .agg(
                     F.count("*").alias("num_records"),
                     F.avg("monthly_income_usd").alias("avg_income"),
                     F.avg("monthly_expenses_usd").alias("avg_expenses"),
                     F.avg("savings_to_income_ratio").alias("avg_savings_ratio")
                 ).orderBy(F.desc("num_records")))

# average loan amount per loan type
loan_agg = (df.groupBy("loan_type")
            .agg(
                F.count("*").alias("num_loans"),
                F.avg("loan_amount_usd").alias("avg_loan"),
                F.avg("monthly_emi_usd").alias("avg_emi")
            ).orderBy(F.desc("num_loans")))

# JOIN
joined = df.join(loan_agg, on="loan_type", how="left") \
           .select("user_id", "education_level", "monthly_income_usd", "loan_type", "loan_amount_usd", "num_loans", "avg_loan")

print("Education Aggregation")
education_agg.show(20, truncate=False)

print("Loan Aggregation")
loan_agg.show(20, truncate=False)

print("Joined Table Sample")
joined.show(20, truncate=False)

# TOP-N LIST
top_savings = df.orderBy(F.desc("savings_usd")).select("user_id", "monthly_income_usd", "savings_usd", "region").limit(10)

# Skinny table
skinny = df.select("user_id", "age", "gender", "education_level", "employment_status",
                   "monthly_income_usd", "monthly_expenses_usd", "savings_usd",
                   "has_loan", "loan_amount_usd", "monthly_emi_usd", "loan_type",
                   "loan_interest_rate_pct", "record_date_parsed")
print("Top 10 Users by Savings")
top_savings.show(truncate=False)

print("Skinny Table Sample")
skinny.show(10, truncate=False)

# Save outputs
# education_agg.write.mode("overwrite").csv(os.path.join(out_dir, "education_agg.parquet"))
# loan_agg.write.mode("overwrite").csv(os.path.join(out_dir, "loan_agg.parquet"))
# joined.write.mode("overwrite").csv(os.path.join(out_dir, "joined_table.parquet"))
# top_savings.write.mode("overwrite").csv(os.path.join(out_dir, "top_savings.parquet"))
# skinny.write.mode("overwrite").csv(os.path.join(out_dir, "skinny_table.parquet"))

education_agg.write.mode("overwrite").parquet(f"{out_dir}/education_agg")
loan_agg.write.mode("overwrite").parquet(f"{out_dir}/loan_agg")
joined.write.mode("overwrite").parquet(f"{out_dir}/joined_table")
top_savings.write.mode("overwrite").parquet(f"{out_dir}/top_savings")
skinny.write.mode("overwrite").parquet(f"{out_dir}/skinny_table")

print("All outputs saved to", out_dir)
#additional analysis ----------------------------------------------------------------
import matplotlib.pyplot as plt
import os
from pyspark.sql import functions as F
import io
from google.cloud import storage

# Filter to male/female only
df_binary_gender = df.filter(F.col("gender").isin("Male", "Female"))

gender_income_global = (
    df_binary_gender.groupBy("gender")
        .agg(F.avg("monthly_income_usd").alias("mean_income_usd"))
        .orderBy(F.desc("mean_income_usd"))
)

gender_income_global_pd = gender_income_global.toPandas()

plt.figure(figsize=(8,6))
plt.bar(gender_income_global_pd["gender"],
        gender_income_global_pd["mean_income_usd"],
        color=["steelblue", "lightcoral"])

plt.title("Global Mean Monthly Income (Male vs Female)")
plt.xlabel("Gender")
plt.ylabel("Mean Monthly Income (USD)")
plt.xticks(rotation=0)

# dynamic y-axis
ymin = 2000
ymax = float(gender_income_global_pd["mean_income_usd"].max())
margin = 0.05 * (ymax - ymin)
plt.ylim(ymin - margin, ymax + margin)

plt.tight_layout()

buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
buf.seek(0)

storage_client = storage.Client()
bucket_obj = storage_client.bucket(bucket)
blob = bucket_obj.blob(f"output/mean_income_male_female_global.png")
blob.upload_from_string(buf.getvalue(), content_type='image/png')
plt.close()

print("Saved chart to: gs://", bucket, "/output/mean_income_male_female_global.png")

import matplotlib.pyplot as plt
import os
from pyspark.sql import functions as F
import io
from google.cloud import storage

# Compute median monthly income by region (values already converted to USD earlier)
region_income_spark = (
    df.groupBy("region")
      .agg(
          F.expr("percentile_approx(monthly_income_usd, 0.5)").alias("median_income_usd")
      )
      .orderBy(F.desc("median_income_usd"))
)

region_income_pd = region_income_spark.toPandas()
region_income_pd = region_income_pd.sort_values(by="median_income_usd", ascending=False)

# Plot the chart
plt.figure(figsize=(12,7))
x = range(len(region_income_pd))
plt.bar(x, region_income_pd["median_income_usd"], color='salmon')
plt.xticks(x, region_income_pd["region"], rotation=45, ha='right')

plt.xlabel("Region")
plt.ylabel("Median Monthly Income (USD)")
plt.title("Median Monthly Income by Region (Converted to USD)")

plt.tight_layout()

buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
buf.seek(0)

storage_client = storage.Client()
bucket_obj = storage_client.bucket(bucket)
blob = bucket_obj.blob(f"output/median_income_by_region_usd.png")
blob.upload_from_string(buf.getvalue(), content_type='image/png')
plt.close()

print("Saved chart to: gs://", bucket, "/output/median_income_by_region_usd.png")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from matplotlib.dates import DateFormatter, AutoDateLocator
import io
from google.cloud import storage
from pyspark.sql import functions as F

monthly_savings = (
    df.groupBy("year_month")
         .agg(F.avg("savings_usd").alias("avg_savings"))
         .orderBy("year_month")
)

monthly_savings_pd = monthly_savings.toPandas()

monthly_savings_pd['year_month_dt'] = pd.to_datetime(monthly_savings_pd['year_month'], format='%Y-%m')
monthly_savings_pd = monthly_savings_pd.dropna().sort_values('year_month_dt')
monthly_savings_pd['avg_savings'] = monthly_savings_pd['avg_savings'].astype(float)

start_inflation = 0
inflation_values = []
current = start_inflation

for _ in range(len(monthly_savings_pd)):
    inflation_values.append(current)
    current += 0.33

inflation_df = pd.DataFrame({
    "year_month_dt": monthly_savings_pd['year_month_dt'].values,
    "inflation_yoy": inflation_values
})

merged = pd.merge(
    monthly_savings_pd,
    inflation_df,
    on="year_month_dt",
    how="left"
)

merged['adj_savings'] = merged['avg_savings'] * (1 - merged['inflation_yoy'] / 100)

plt.figure(figsize=(14,7))
ax = plt.gca()

sns.lineplot(data=merged, x='year_month_dt', y='avg_savings',
             label='Average Savings', marker='o')

sns.lineplot(data=merged, x='year_month_dt', y='adj_savings',
             label='Inflation-Adjusted Savings',
             color='red', marker='s')

ax.fill_between(
    merged['year_month_dt'],
    merged['adj_savings'],
    merged['avg_savings'],
    where=merged['avg_savings'] >= merged['adj_savings'],
    color='red',
    alpha=0.2
)

ax.set_xlabel("Month")
ax.set_ylabel("Savings (USD)")
ax.tick_params(axis='x', rotation=45)
ax.xaxis.set_major_locator(AutoDateLocator())
ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))

plt.title('Average Savings vs Inflation-Adjusted Savings (2020-2025)')

plt.tight_layout()
buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
buf.seek(0)

storage_client = storage.Client()
bucket_obj = storage_client.bucket(bucket)
blob = bucket_obj.blob(f"output/na_savings_inflation_rising.png")
blob.upload_from_string(buf.getvalue(), content_type='image/png')
plt.close()

print("Chart saved to: gs://", bucket, f"/output/na_savings_inflation_rising.png")
